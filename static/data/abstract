Current audio foundation models typically rely on rigid, task-specific supervision, addressing isolated factors of audio rather than the whole. In contrast, human intelligence processes audio holistically, seamlessly bridging physical signals with abstract cognitive concepts to execute complex tasks. Grounded in this philosophy, we introduce \texttt{Bagpiper}, an 8B audio foundation model that interprets physical audio via \textit{rich captions}, i.e., comprehensive natural language descriptions that encapsulate the critical cognitive concepts inherent in the signal (e.g., transcription, audio events). By pre-training on a massive corpus of 600B tokens, the model establishes a robust bidirectional mapping between raw audio and this high-level conceptual space. During fine-tuning, \texttt{Bagpiper} adopts a \textit{caption-then-process} workflow, simulating an intermediate cognitive reasoning step to solve diverse tasks without task-specific priors. Experimentally, \texttt{Bagpiper} outperforms Qwen-2.5-Omni on MMAU and AIR-Bench for audio understanding and surpasses CosyVoice3 and TangoFlux in generation quality, capable of synthesizing arbitrary compositions of speech, music, and sound effects. 
To the best of our knowledge, \texttt{Bagpiper} is among the first works that achieve unified understanding-generation for general audio.